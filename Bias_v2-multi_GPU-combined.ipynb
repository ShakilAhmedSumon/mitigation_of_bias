{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deecfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an updated version of the report attched in the repo\n",
    "# It investigates three bias dimensionsns race, religion and gender (male and female)\n",
    "# with only one metric: False Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba71e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93b83bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_data = pd.read_csv(\"data/path/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25a9d0b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6944"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_data[religion_data['target'] == 1]['comment_text'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3551124e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47389"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_data[religion_data['target'] == 0]['comment_text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ee96652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the dataset into two groups\n",
    "group_0 = religion_data[religion_data['target'] == 0]\n",
    "group_1 = religion_data[religion_data['target'] == 1]\n",
    "\n",
    "# Count the number of instances in each group\n",
    "count_group_0 = group_0.shape[0]\n",
    "count_group_1 = group_1.shape[0]\n",
    "\n",
    "# Resample the larger group\n",
    "if count_group_0 > count_group_1:\n",
    "    group_0 = group_0.sample(count_group_1)  # Downsample group 0\n",
    "else:\n",
    "    group_1 = group_1.sample(count_group_0)  # Downsample group 1\n",
    "\n",
    "# Concatenate the resampled groups\n",
    "balanced_religion_data = pd.concat([group_0, group_1])\n",
    "\n",
    "# Shuffle the balanced dataset\n",
    "balanced_religion_data = balanced_religion_data.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efef1173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6944\n",
       "1    6944\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_religion_data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e45a99ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7888\n",
       "1    6000\n",
       "Name: religion, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_religion_data['religion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e306d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70eb0bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"path/to/tran/data\")\n",
    "val_data = pd.read_csv(\"path/to/val/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc0ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636c5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = df['hate_speech'].values\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['comment']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e01b824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246 1000\n"
     ]
    }
   ],
   "source": [
    "df_train = train_data\n",
    "df_val = val_data\n",
    "\n",
    "\n",
    "print(len(df_train),len(df_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa1ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(768, 768)\n",
    "        self.linear2 = nn.Linear(768, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear1(dropout_output)\n",
    "        linear_output = self.linear2(linear_output)\n",
    "        final_layer = self.sigmoid(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "323a74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "#     print('Labels: ', y)\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "#     print('Rounded preds',rounded_preds)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division\n",
    "#     print('Correct: ',correct)\n",
    "    acc = correct.sum() / len(correct)\n",
    "#     print('Length: ',len(correct))\n",
    "#     print('Sum of correct: ', correct.sum())\n",
    "#     print('Accuracy: ', acc.item())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aa7630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=24, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=24)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "    val_loss_extreme = 100000\n",
    "\n",
    "    if use_cuda:\n",
    "            model= nn.DataParallel(model)\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            correct_train_sum = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.unsqueeze(1).to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                \n",
    "#                 print('Output Shape: ', output.shape)\n",
    "#                 print('train label shape: ', train_label.shape)\n",
    "                \n",
    "#                 print(type(output))\n",
    "#                 print(type(train_label))\n",
    "                train_label = train_label.to(torch.float32)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label)\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                ## acc calc\n",
    "                \n",
    "                \n",
    "                rounded_preds = torch.round(output)\n",
    "                correct_train = (rounded_preds == train_label).float()\n",
    "                correct_train_sum += correct_train.sum()\n",
    "#                 print(correct_train_sum)\n",
    "                \n",
    "                \n",
    "                ##\n",
    "                \n",
    "#                 acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "#                 acc = binary_accuracy(output, train_label)\n",
    "#                 total_acc_train += acc.item()\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "#             print((correct_train_sum/len(train_data)))\n",
    "            train_acc = (correct_train_sum/len(train_data)).item()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            correct_val_sum = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.unsqueeze(1).to(device)\n",
    "                    val_label = val_label.to(torch.float32)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label)\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    rounded_preds = torch.round(output)\n",
    "                    correct_val = (rounded_preds == val_label).float()\n",
    "                    correct_val_sum += correct_val.sum()\n",
    "                    \n",
    "#                     acc = binary_accuracy(output, val_label)\n",
    "#                     total_acc_val += acc.item()\n",
    "            \n",
    "            \n",
    "            val_acc = (correct_val_sum/len(val_data)).item()\n",
    "            if total_loss_val < val_loss_extreme:\n",
    "                val_loss_extreme = total_loss_val\n",
    "                torch.save(model.state_dict(), '/nfs/hpc/share/sumons/bias-bert-model_mlg_combined.pt')\n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} | Train Accuracy: {train_acc: .3f} | Val Loss: {total_loss_val / len(val_data): .3f} | Val Accuracy: {val_acc: .3f}')\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21323f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1261/1261 [11:09<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.027 | Train Accuracy:  0.659 | Val Loss:  0.025 | Val Accuracy:  0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.025 | Train Accuracy:  0.802 | Val Loss:  0.025 | Val Accuracy:  0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:18<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.024 | Train Accuracy:  0.826 | Val Loss:  0.025 | Val Accuracy:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.024 | Train Accuracy:  0.847 | Val Loss:  0.025 | Val Accuracy:  0.820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:17<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.024 | Train Accuracy:  0.860 | Val Loss:  0.025 | Val Accuracy:  0.816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:18<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.024 | Train Accuracy:  0.869 | Val Loss:  0.025 | Val Accuracy:  0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.023 | Train Accuracy:  0.874 | Val Loss:  0.025 | Val Accuracy:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:18<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.023 | Train Accuracy:  0.880 | Val Loss:  0.025 | Val Accuracy:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.023 | Train Accuracy:  0.884 | Val Loss:  0.025 | Val Accuracy:  0.810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:17<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.023 | Train Accuracy:  0.888 | Val Loss:  0.025 | Val Accuracy:  0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.023 | Train Accuracy:  0.890 | Val Loss:  0.025 | Val Accuracy:  0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.023 | Train Accuracy:  0.893 | Val Loss:  0.025 | Val Accuracy:  0.815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:18<00:00,  1.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.023 | Train Accuracy:  0.895 | Val Loss:  0.025 | Val Accuracy:  0.818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:20<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.023 | Train Accuracy:  0.896 | Val Loss:  0.025 | Val Accuracy:  0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261/1261 [11:21<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.023 | Train Accuracy:  0.896 | Val Loss:  0.025 | Val Accuracy:  0.814\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_val, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2522883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=12)\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "\n",
    "    total_acc_test = 0\n",
    "    correct_test_sum = 0\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.unsqueeze(1).to(device)\n",
    "            test_label = test_label.to(torch.float32)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "            output = model(input_id, mask)\n",
    "            \n",
    "            rounded_preds = torch.round(output)\n",
    "            for pred in rounded_preds:\n",
    "                predictions.append(int(pred.item()))\n",
    "                \n",
    "            correct_test = (rounded_preds == test_label).float()\n",
    "            correct_test_sum += correct_test.sum()\n",
    "    \n",
    "\n",
    "    test_acc = (correct_test_sum/len(test_data)).item()\n",
    "    print(f'Test Accuracy: {test_acc: .3f}')\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "233fb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the hold-out test data for each of the bias dimension\n",
    "\n",
    "test_gender = pd.read_csv(\"/test/gender/data\")\n",
    "test_religion = pd.read_csv(\"/test/religion/data\")\n",
    "test_race = pd.read_csv(\"/test/race/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebab9ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.804\n",
      "Test Accuracy:  0.774\n",
      "Test Accuracy:  0.727\n"
     ]
    }
   ],
   "source": [
    "model = BertClassifier()\n",
    "model= nn.DataParallel(model)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load('/bias-bert-model_mlg_combined.pt'))\n",
    "\n",
    "predictions_gender = evaluate(model, test_gender)\n",
    "predictions_religion = evaluate(model, test_religion)\n",
    "predictions_race = evaluate(model, test_race)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa23bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test for bias by seeing the false negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd2464",
   "metadata": {},
   "source": [
    "### Case: Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f21dfa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gender['predictions'] = predictions_gender\n",
    "test_religion['predictions'] = predictions_religion\n",
    "test_race['predictions'] = predictions_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c71f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_muslim = test_religion[test_religion['muslim'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d651b474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>comment</th>\n",
       "      <th>muslim</th>\n",
       "      <th>christian</th>\n",
       "      <th>religion</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Just more misinformed right wingers, got their...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>When the arguments were being made at the lowe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Wake the hell up, ISIS is at war with us.  The...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>2 major terrorist attacks today -- the fear do...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>Get some muslims in there for their starting r...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>1</td>\n",
       "      <td>Another Muslim attacking Americans. When will ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear authors of this liberal-biased drivel, th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>Save your time and money, Erdogan. The whole w...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>1</td>\n",
       "      <td>no actually we gave women suffrage to be thems...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1</td>\n",
       "      <td>Maybe you could also think about the home-grow...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hate_speech                                            comment  muslim  \\\n",
       "0              0  Just more misinformed right wingers, got their...       1   \n",
       "4              0  When the arguments were being made at the lowe...       1   \n",
       "7              0  Wake the hell up, ISIS is at war with us.  The...       1   \n",
       "9              0  2 major terrorist attacks today -- the fear do...       1   \n",
       "10             0  Get some muslims in there for their starting r...       1   \n",
       "..           ...                                                ...     ...   \n",
       "494            1  Another Muslim attacking Americans. When will ...       1   \n",
       "495            1  Dear authors of this liberal-biased drivel, th...       1   \n",
       "496            1  Save your time and money, Erdogan. The whole w...       1   \n",
       "497            1  no actually we gave women suffrage to be thems...       1   \n",
       "498            1  Maybe you could also think about the home-grow...       1   \n",
       "\n",
       "     christian  religion  predictions  \n",
       "0            0         1            1  \n",
       "4            0         1            0  \n",
       "7            0         1            1  \n",
       "9            0         1            0  \n",
       "10           0         1            1  \n",
       "..         ...       ...          ...  \n",
       "494          0         1            1  \n",
       "495          0         1            1  \n",
       "496          0         1            1  \n",
       "497          0         1            1  \n",
       "498          0         1            0  \n",
       "\n",
       "[211 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_muslim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb682187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>comment</th>\n",
       "      <th>muslim</th>\n",
       "      <th>christian</th>\n",
       "      <th>religion</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ah, so you are a Cafeteria Catholic.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>contact Catholic Ann Barnhardt</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>The reason for focus on chapter eight is that ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>\"...to find good married couples, and draw pri...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>No. \"Conservatives,\" as you call orthodox Cath...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>1</td>\n",
       "      <td>I don’t understand how Pope F. could talk abou...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>1</td>\n",
       "      <td>The one thing DEMS had in common is they didn'...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>1</td>\n",
       "      <td>Timothy McVeigh was a Roman Catholic and he ki...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Eight years ago, when Barack Obama was electe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1</td>\n",
       "      <td>You prove my point that it is a matter of pers...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     hate_speech                                            comment  muslim  \\\n",
       "1              0               Ah, so you are a Cafeteria Catholic.       0   \n",
       "2              0                     contact Catholic Ann Barnhardt       0   \n",
       "3              0  The reason for focus on chapter eight is that ...       0   \n",
       "5              0  \"...to find good married couples, and draw pri...       0   \n",
       "6              0  No. \"Conservatives,\" as you call orthodox Cath...       0   \n",
       "..           ...                                                ...     ...   \n",
       "485            1  I don’t understand how Pope F. could talk abou...       0   \n",
       "486            1  The one thing DEMS had in common is they didn'...       0   \n",
       "487            1  Timothy McVeigh was a Roman Catholic and he ki...       0   \n",
       "490            1  \"Eight years ago, when Barack Obama was electe...       0   \n",
       "499            1  You prove my point that it is a matter of pers...       0   \n",
       "\n",
       "     christian  religion  predictions  \n",
       "1            1         0            0  \n",
       "2            1         0            0  \n",
       "3            1         0            0  \n",
       "5            1         0            0  \n",
       "6            1         0            0  \n",
       "..         ...       ...          ...  \n",
       "485          1         0            1  \n",
       "486          1         0            1  \n",
       "487          1         0            0  \n",
       "490          1         0            1  \n",
       "499          1         0            1  \n",
       "\n",
       "[289 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_christian = test_religion[test_religion['christian'] == 1]\n",
    "df_test_christian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b1e4ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.3181818181818182\n"
     ]
    }
   ],
   "source": [
    "# Calculate false positives for male\n",
    "false_positives_muslim = ((df_test_muslim['hate_speech'] == 0) & (df_test_muslim['predictions'] == 1)).sum()\n",
    "\n",
    "# Calculate total actual negatives\n",
    "total_actual_negatives_muslim = (df_test_muslim['hate_speech'] == 0).sum()\n",
    "\n",
    "# Calculate false positive rate\n",
    "false_positive_rate_muslim = false_positives_muslim / total_actual_negatives_muslim\n",
    "\n",
    "# Print the false positive rate\n",
    "print(f\"False Positive Rate: {false_positive_rate_muslim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1913ea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.08695652173913043\n"
     ]
    }
   ],
   "source": [
    "# Calculate false positives for female\n",
    "false_positives_christian = ((df_test_christian['hate_speech'] == 0) & (df_test_christian['predictions'] == 1)).sum()\n",
    "\n",
    "# Calculate total actual negatives\n",
    "total_actual_negatives_christian = (df_test_christian['hate_speech'] == 0).sum()\n",
    "\n",
    "# Calculate false positive rate\n",
    "false_positive_rate_christian = false_positives_christian / total_actual_negatives_christian\n",
    "\n",
    "# Print the false positive rate\n",
    "print(f\"False Positive Rate: {false_positive_rate_christian}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa426e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c5e3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a960ff8c",
   "metadata": {},
   "source": [
    "### Case: Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d0327b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_female = test_gender[test_gender['female'] == 1]\n",
    "df_test_male = test_gender[test_gender['male'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "598ef1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_male['hate_speech'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "303684ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_female = df_test_female[:216]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5375d460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_female['hate_speech'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f85f91c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.14965986394557823\n"
     ]
    }
   ],
   "source": [
    "# Calculate false positives for male\n",
    "false_positives_female = ((df_test_female['hate_speech'] == 0) & (df_test_female['predictions'] == 1)).sum()\n",
    "\n",
    "# Calculate total actual negatives\n",
    "total_actual_negatives_female = (df_test_female['hate_speech'] == 0).sum()\n",
    "\n",
    "# Calculate false positive rate\n",
    "false_positive_rate_female = false_positives_female / total_actual_negatives_female\n",
    "\n",
    "# Print the false positive rate\n",
    "print(f\"False Positive Rate: {false_positive_rate_female}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0dacff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.17475728155339806\n"
     ]
    }
   ],
   "source": [
    "# Calculate false positives for male\n",
    "false_positives_male = ((df_test_male['hate_speech'] == 0) & (df_test_male['predictions'] == 1)).sum()\n",
    "\n",
    "# Calculate total actual negatives\n",
    "total_actual_negatives_male = (df_test_male['hate_speech'] == 0).sum()\n",
    "\n",
    "# Calculate false positive rate\n",
    "false_positive_rate_male = false_positives_male / total_actual_negatives_male\n",
    "\n",
    "# Print the false positive rate\n",
    "print(f\"False Positive Rate: {false_positive_rate_male}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e2a2e5",
   "metadata": {},
   "source": [
    "### Case: Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fc8a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_white = test_race[test_race['white'] == 1]\n",
    "df_test_black = test_race[test_race['black'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdcaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d77707f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_white = df_test_white[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5dc279cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_black = df_test_black[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf12a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.2935323383084577\n"
     ]
    }
   ],
   "source": [
    "# Calculate false positives for male\n",
    "false_positives_white = ((df_test_white['hate_speech'] == 0) & (df_test_white['predictions'] == 1)).sum()\n",
    "\n",
    "# Calculate total actual negatives\n",
    "total_actual_negatives_white = (df_test_white['hate_speech'] == 0).sum()\n",
    "\n",
    "# Calculate false positive rate\n",
    "false_positive_rate_white = false_positives_white / total_actual_negatives_white\n",
    "\n",
    "# Print the false positive rate\n",
    "print(f\"False Positive Rate: {false_positive_rate_white}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc0ab0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate: 0.15428571428571428\n"
     ]
    }
   ],
   "source": [
    "# Calculate false positives for male\n",
    "false_positives_black = ((df_test_black['hate_speech'] == 0) & (df_test_black['predictions'] == 1)).sum()\n",
    "\n",
    "# Calculate total actual negatives\n",
    "total_actual_negatives_black = (df_test_black['hate_speech'] == 0).sum()\n",
    "\n",
    "# Calculate false positive rate\n",
    "false_positive_rate_black = false_positives_black / total_actual_negatives_black\n",
    "\n",
    "# Print the false positive rate\n",
    "print(f\"False Positive Rate: {false_positive_rate_black}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e9417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
